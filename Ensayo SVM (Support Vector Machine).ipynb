{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensayo - Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepto \n",
    "\n",
    "Support Vector Machine  (SVM) son un conjunto de métodos de aprendizaje supervisado utilizados para la clasificación, regresión y detección de valores atípicos. El SVM básico toma un conjunto de datos de entrada y predice, para cada entrada dada, a cuál de las dos clases de salida pertenece, por lo que es un clasificador no-probabilístico lineal binario (solo escoge entre 2 opciones). \n",
    "\n",
    "Si bien originariamente se desarrolló como un método de clasificación binaria, su aplicación se ha extendido a problemas de clasificación múltiple y regresión. SVM ha resultado ser uno de los mejores clasificadores para un amplio abanico de situaciones, por lo que se considera uno de los referentes dentro del ámbito de aprendizaje estadístico y machine learning.\n",
    "\n",
    "SVMs se fundamentan en el Maximal Margin Classifier, que a su vez, se basa en el concepto de hiperplano.*(Ambos conceptos se explican más adelante)*.\n",
    "\n",
    "\n",
    "Las ventajas de las máquinas de vectores de soporte son:\n",
    "* Efectivo en espacios de altas dimensiones.\n",
    "* Sigue siendo efectivo en casos donde el número de dimensiones es mayor que el número de muestras.\n",
    "* Utiliza un subconjunto de puntos de entrenamiento en la función de decisión (llamados vectores de soporte), por lo que también es eficiente en la memoria.\n",
    "* Versátil: se pueden especificar diferentes funciones de Kernel para la función de decisión. Se proporcionan núcleos comunes, pero también es posible especificar núcleos personalizados.\n",
    "\n",
    "Las desventajas de las máquinas de vectores de soporte incluyen:\n",
    "* Si el número de características es mucho mayor que el número de muestras, evite el ajuste excesivo al elegir las funciones del núcleo y el término de regularización es crucial.\n",
    "* Los SVM no proporcionan directamente estimaciones de probabilidad, estas se calculan utilizando una costosa validación cruzada de cinco veces (ver Puntajes y probabilidades, a continuación).\n",
    "* No muy eficiente en muestras con muchos datos, dado el tiempo de procesamiento del algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperplano \n",
    "\n",
    "En SVM se traza cada observación como un punto en un espacio dimensional *n* donde *n* es el número de características que tenemos. El valor de cada característica es el valor de coordenadas particulares. Luego se intenta encontrar un hiper-plano que separe muy bien dos clases.\n",
    "\n",
    "<img src=\"svm_assets/ImageSVM_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de identificar el mejor hiper-plano, buscaremos agregar márgenes que ayudarían a separar más aún las dos clases.\n",
    "\n",
    "<img src=\"svm_assets/ImageSVM_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM es muy efectivo cuando el número de características es muy alto o si el número de características es más mayor al número de muestras de datos. Aunque, debido a que SVM funciona normalmente con vectores, es crucial normalizar los datos antes de usarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El hecho de que los grupos no sean linealmente separables en el espacio original no significa que no lo sean en un espacio de mayores dimensiones. Las imágenes siguientes muestran como dos grupos, cuya separación en dos dimensiones no es lineal, sí lo es al añadir una tercera dimensión.\n",
    "\n",
    "<img src=\"svm_assets/ImageSVM_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Lineal\n",
    "\n",
    "Comencemos desde SVM lineal que se conoce como SVM sin núcleos (kernels). Mirando el diagrama de dispersión por dos características X1, X2 como se muestra a continuación. En realidad, separamos dos clases de muchas maneras diferentes, la línea rosa y la línea verde son dos de ellas. SVM termina eligiendo la línea verde como límite de decisión, porque la forma en que SVM clasifica las muestras es encontrar el límite de decisión con el margen más grande que es la distancia más grande de una muestra que está más cerca del límite de decisión. Es por eso que **Linear SVM** también se llama Clasificador de margen grande *(Large Margin Classifier)*.\n",
    "\n",
    "<img src=\"svm_assets/ImageSVM_4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*¿Quiénes son los vectores de soporte?* El vector de soporte es una muestra que está clasificada incorrectamente o una muestra cercana a un límite. Mirando la trama a continuación. Las muestras con círculos rojos son exactamente el límite de decisión. En SVM, solo los vectores de soporte tienen un impacto efectivo en el entrenamiento del modelo, es decir, eliminar el vector sin soporte no tiene ningún efecto en el modelo. \n",
    "\n",
    "<img src=\"svm_assets/ImageSVM_5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Hipotesis\n",
    "\n",
    "La función de hipotesis para SVM no es mas que:\n",
    "\n",
    "$$h_\\theta(x) = \\left\\{\n",
    "                \\begin{array}\\\\\n",
    "                    1 & \\mbox{si } \\ \\theta^Tx >= 0 \\\\\n",
    "                    0 & \\mbox{otherwise } \n",
    "                \\end{array}\n",
    "             \\right.$$\n",
    "             \n",
    "             \n",
    "Si la distribución de las observaciones es tal que se pueden separar linealmente de forma perfecta en las dos clases (+1 y 0), entonces, un hiperplano de separación cumple que:\n",
    "\n",
    "$$\\theta_0+\\theta_1x_1+\\theta_2x_2+ ... +\\theta_nx_n > 0, \\mbox{si} \\ y_i = 1\\\\.$$\n",
    "$$\\theta_0+\\theta_1x_1+\\theta_2x_2+ ... +\\theta_nx_n < 0, \\mbox{si} \\ y_i = 0\\\\.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Costo (Hinge Loss)\n",
    "\n",
    "La función de pérdida de SVM es muy similar a la de Regresión logística. Mirándolo por y = 1 e y = 0 por separado en la siguiente gráfica, la línea negra es la función de costo de la Regresión logística, y la línea roja es para SVM. Tenga en cuenta que el eje X aquí es la salida del modelo en bruto, **θᵀx**. Recuerde que poner la salida del modelo en bruto en la función Sigmoide nos da la hipótesis de la regresión logística. ¿Cuál es la hipótesis para SVM? Es simple y directo. Cuando **θᵀx ≥ 0**, predice 1, de lo contrario, predice 0.\n",
    "\n",
    "<img src=\"svm_assets/ImageSVM_6.png\">\n",
    " \n",
    "SVM castiga tanto las predicciones incorrectas como las cercanas al límite de decisión (0 <θᵀx <1), así es como los llamamos vectores de soporte. \n",
    " \n",
    "Escribamos la formula de la función de costo:\n",
    "$$Cost(h_\\theta(x),y) = \\left\\{\n",
    "                \\begin{array}\\\\\n",
    "                    max(0,1-\\theta^Tx) & \\mbox{si } \\ y = 1 \\\\\n",
    "                    max(0,1+\\theta^Tx) & \\mbox{si } \\ y = 0 \\\\\n",
    "                \\end{array}\n",
    "             \\right.$$\n",
    "\n",
    "\n",
    "Sin regularización la función de costo quedaría:\n",
    "$$ J(\\theta) = \\sum^m_{i=1}y^{(i)}max\\{0,1 - \\theta^Tx) + (1-y^{(i)})max\\{0,1 + \\theta^Tx)$$\n",
    "\n",
    "Con regularización la función de costo quedaría:\n",
    "$$ J(\\theta) = 1/2\\sum^n_{j=1}\\theta_j^2 + C[\\sum^m_{i=1}y^{(i)}max\\{0,1 - \\theta^Tx) + (1-y^{(i)})max\\{0,1 + \\theta^Tx)]$$\n",
    "\n",
    "donde *m= Número de muestras* y *n= número de features*\n",
    "\n",
    "Función de Costo obtenida del video:  \n",
    "https://www.youtube.com/watch?v=geI6lM5iOl0&feature=youtu.be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferencias con Modelo Regresión Lineal\n",
    "\n",
    "Como indicamos anteriormente, el modelo SVM lineal es muy parecido al modelo de regresión logistica, pero a a diferencia de la regresión logística que usa λ como parámetro delante del término regularizado para controlar el peso de la regularización; en consecuencia, SVM usa C delante del término de ajuste.\n",
    "\n",
    "Intuitivamente, el término de ajuste enfatiza el ajuste del modelo muy bien al encontrar coeficientes óptimos, y el término regularizado controla la complejidad del modelo al restringir el gran valor de los coeficientes. Existe una compensación entre ajustar bien el modelo en el conjunto de datos de entrenamiento y la complejidad del modelo que puede conducir a un sobreajuste, que puede ajustarse ajustando el valor de λ o C. \n",
    "\n",
    "Tanto λ como C priorizan cuánto nos importa optimizar término apto y plazo regularizado. Colocando en diferentes lugares de función de costo, C en realidad juega un papel similar a 1 / λ.  \n",
    "\n",
    "Con un valor muy grande de C (similar a la no regularización), este clasificador de margen grande será muy sensible a los valores atípicos. Por ejemplo, en la gráfica de la izquierda, como se muestra a continuación, el límite de decisión ideal debe ser como una línea verde, al agregar el triángulo naranja anaranjado (atípico), con una C muy grande, el límite de decisión se desplazará a la línea naranja para satisfacer el La regla del gran margen. Por otro lado, C también juega un papel para ajustar el ancho del margen que permite la violación del margen. Vea la trama a continuación a la derecha. Cuando C es pequeño, el margen se muestra más ancho como línea verde. Los puntos de datos rosados han violado el margen. Es especialmente útil cuando se trata de un conjunto de datos no separable. Así es como la regularización impacta la elección del límite de decisión que hace que el algoritmo funcione para un conjunto de datos separable no linealmente con tolerancia de puntos de datos que están mal clasificados o tienen violación de margen.\n",
    "\n",
    "<img src=\"svm_assets/ImageSVM_7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM No Lineal - Uso de Nucleos (Kernels)\n",
    "\n",
    "Antes de adentrarnos en el concepto de uso de kernel mostraremos que la función de hipotesis y costo en estructura es similar. Veamos:\n",
    "\n",
    "### Función de Costo\n",
    "\n",
    "$$h_\\theta(x) = \\left\\{\n",
    "                \\begin{array}\\\\\n",
    "                    1 & \\mbox{si } \\ \\theta^Tf >= 0 \\\\\n",
    "                    0 & \\mbox{otherwise } \n",
    "                \\end{array}\n",
    "             \\right.$$\n",
    "\n",
    "### Función de Costo\n",
    "\n",
    "Sin regularización la función de costo quedaría:\n",
    "$$ J(\\theta) = \\sum^m_{i=1}y^{(i)}max\\{0,1 - \\theta^Tf) + (1-y^{(i)})max\\{0,1 + \\theta^Tf)$$\n",
    "\n",
    "Con regularización la función de costo quedaría:\n",
    "$$ J(\\theta) = 1/2\\sum^m_{j=1}\\theta_j^2 + C[\\sum^m_{i=1}y^{(i)}max\\{0,1 - \\theta^Tf) + (1-y^{(i)})max\\{0,1 + \\theta^Tf)]$$\n",
    "\n",
    "### Diferencia con SVM Linear\n",
    "\n",
    "Es posible que haya notado que la hipótesis de SVM no lineal y la función de costo son casi las mismas que las SVM lineales, excepto que aquí \"x\" se reemplaza por \"f\". f es la función Nucleo **(Kernel Function)**\n",
    "\n",
    "*Notar que la regularización es diferente, ya que será un theta por cada muestra y no por feature.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Núcleo (Kernel Function)\n",
    "\n",
    "Para mejorar la explicación, supongamos que tenemos una muestra (veamos la gráfica a continuación) con dos características x1, x2. Al azar se pusieron algunos puntos (l⁽¹⁾, l⁽²⁾, l⁽³⁾) alrededor de x, y los llamaremos puntos de referencia. Ahora, me gustaría ver qué tan cerca está x de estos puntos de referencia respectivamente, que se observa como f1 = Similitud (x, l⁽¹⁾) o k (x, l⁽¹⁾), f2 = Similitud (x, l⁽²⁾ ) o k (x, l⁽²⁾), f3 = Similitud (x, l⁽³⁾) o k (x, l⁽³⁾).\n",
    "\n",
    "<img src=\"svm_assets/ImageSVM_8.png\">\n",
    "\n",
    "\n",
    "¿Qué hay dentro de la función Kernel? En pocas palabras describe describe la proximidad de x a los puntos de referencia.\n",
    "\n",
    "Hay diferentes tipos de Kernels, entre ellos se encuentra **Gaussian Kernel** (es uno de los más populares). Se calcula con la distancia euclidiana de dos vectores y el parámetro σ que describe la suavidad de la función. \n",
    "\n",
    "El Kernel gaussiano proporciona una buena intuición. Si x ≈ l⁽¹⁾, f1 ≈ 1, si x está lejos de l⁽¹⁾, f1 ≈ 0. \n",
    "\n",
    "**Nota:** *En el paquete Scikit-learn SVM, el núcleo gaussiano se asigna a 'rbf'(radial basis function kernel), la única diferencia es 'rbf' usa γ para representar el 1 / 2σ² de Gaussian.*\n",
    "\n",
    "$$f_1 = Similarity(x,l^{1}) = \\exp(\\frac{\\begin{Vmatrix}x - l^{1}\\end{Vmatrix}^2}{2\\sigma^2}$$\n",
    "$$f_2 = Similarity(x,l^{1}) = \\exp(\\frac{\\begin{Vmatrix}x - l^{2}\\end{Vmatrix}^2}{2\\sigma^2}$$\n",
    "$$f_3 = Similarity(x,l^{1}) = \\exp(\\frac{\\begin{Vmatrix}x - l^{3}\\end{Vmatrix}^2}{2\\sigma^2}$$\n",
    "\n",
    "Podemos decir que la posición de la muestra x ha sido redefinida por esos tres núcleos. Es decir, SVM no lineal calcula nuevas características f1, f2, f3, dependiendo de la proximidad a los puntos de referencia, en lugar de usar x1, x2 como características, y eso lo deciden los puntos de referencia elegidos. Aquí es de donde proviene la salida del modelo sin procesar θᵀf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a lo anterior, se deben construir limites de decisión como se muestran la siguiente grafica.\n",
    "<img src=\"svm_assets/ImageSVM_9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuántos puntos de referencia necesitamos? Aunque suene sorprendente, dado m muestras de entrenamiento, la ubicación de los puntos de referencia es exactamente la ubicación de sus m muestras de entrenamiento.\n",
    "\n",
    "$$ Dado    (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),.....,(x^{(m)},y^{(m)})$$\n",
    "$$ Tenemos l^{(1)} = x^{(1)},l^{(2)} = x^{(2)},.....,l^{(m)} = x^{(m)}$$\n",
    "\n",
    "Es decir, SVM no lineal recrea las características comparando cada una de sus muestras de entrenamiento con todas las demás muestras de entrenamiento. Por lo tanto, el número de características para la predicción creadas por los puntos de referencia es el tamaño de las muestras de entrenamiento. Para una muestra dada, hemos actualizado las características a continuación:\n",
    "\n",
    "$$f_1^{(i)}=k(x^{(i)}, l^{(1)})$$\n",
    "$$f_2^{(i)}=k(x^{(i)}, l^{(2)})$$\n",
    "$$....$$\n",
    "$$f_i^{(i)}=k(x^{(i)}, l^{(i)})$$\n",
    "$$....$$\n",
    "$$f_m^{(i)}=k(x^{(i)}, l^{(m)})$$\n",
    "\n",
    "$$Donde x^{(i)} = l^{(i)}, f_i^{(i)} = 1$$\n",
    "\n",
    "Con respecto a la recreación de características, este concepto es así cuando se crea una regresión polinómica para alcanzar un efecto no lineal, podemos agregar algunas características nuevas haciendo algunas transformaciones a las características existentes, como el cuadrado. Por ejemplo, tiene dos funciones x1 y x2. Para crear una regresión polinómica, creó θ0 + θ1x1 + θ2x2 + θ3x1² + θ4x1²x2, de modo que sus características se conviertan en f1 = x1, f2 = x2, f3 = x1², f4 = x1²x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recomendaciones Modelo no lineal\n",
    "\n",
    "Para lograr un buen rendimiento del modelo y evitar el sobreajuste, además de elegir un valor adecuado del término regularizado C, también podemos ajustar σ² desde Gaussian Kernel para encontrar el equilibrio entre sesgo y varianza. Tome una determinada muestra xy cierto hito l como ejemplo, cuando σ² es muy grande, la salida de la función del núcleo f está cerca de 1, ya que σ² se hace más pequeña, f se mueve hacia 0. En otras palabras, con una distancia fija entre x y l, una gran σ² lo considera \"más cercano\", que tiene un mayor sesgo y una menor varianza (subadaptación), mientras que una pequeña σ² lo considera \"más\", que tiene un mayor sesgo y una mayor varianza (sobreajuste).\n",
    "\n",
    "Al igual que la Regresión logística, la función de costo de SVM también es convexa. El algoritmo de optimización más popular para SVM es la optimización mínima secuencial que se puede implementar mediante el paquete \"libsvm\" en Python. SMO resuelve un gran problema de programación cuadrática (QP) al dividirlos en una serie de pequeños problemas de QP que pueden resolverse analíticamente para evitar en algún grado el proceso que consume mucho tiempo. En términos de cálculos detallados, es bastante complicado y contiene muchos trucos informáticos numéricos que hacen que los cálculos sean mucho más eficientes para manejar conjuntos de datos de entrenamiento muy grandes.\n",
    "\n",
    "En resumen, si tiene una gran cantidad de funciones, probablemente sea una opción SVM lineal o Regresión logística. Si tiene un pequeño número de funciones (menos de 1000) y muestras de entrenamiento de un tamaño no demasiado grande, SVM con Gaussian Kernel podría funcionar bien para sus datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
